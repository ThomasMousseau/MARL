{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2iXfk_yueBG"
   },
   "source": [
    "##### INF8250 – Reinforcement Learning - Fall 2024 - Final Project\n",
    "\n",
    "# **Multi-Agent Reinforcement Learning**\n",
    "\n",
    "### Members:\n",
    "\n",
    "- Alexandre Fournier - 2147771\n",
    "\n",
    "- Thomas Mousseau - 2149672\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkcXOcIvvc70"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Multi-Agent Reinforcement Learning (MARL) extends the RL framework to environments where multiple agents interact, each potentially learning and adapting simultaneously. In MARL, agents may cooperate, compete, or both, adding complexity to the learning process. Understanding MARL is crucial for developing systems where multiple autonomous entities need to operate in shared environments, such as swarm robotics, autonomous driving, and distributed control systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v4BCtgvvk0q"
   },
   "source": [
    "## 1. From Single-Agent MDP to Multi-Agent Markov Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRX136vA1I6j"
   },
   "source": [
    "### Markov Decision Process (MDP)\n",
    "\n",
    "In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment with the objective of maximizing the expected cumulative reward over time. The framework is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, r, \\gamma)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{S} &: \\text{ States of the environment} \\\\\n",
    "\\mathcal{A} &: \\text{ Actions set of the agent } \\\\\n",
    "\\mathcal{a} &: \\text{ The action taken by the agent in state s } \\\\\n",
    "\\mathcal{P}(s' \\mid s, a) &: \\text{ Transition probability for reaching state } s' \\text{ from } s \\text{ after action } a \\\\\n",
    "r(s, a) &: \\text{ Reward for taking action } a \\text{ in state } s \\\\\n",
    "\\gamma &: \\text{ Discount factor, determining the importance of future rewards }\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The agent’s policy function $\\pi(a \\mid s)$ defines the probability of selecting action $(a)$ in state $(s)$, while the value function $V(s)$ represents the expected cumulative reward starting from $(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhSmGEKH0j1Q"
   },
   "source": [
    "### Markov Game (MG)\n",
    "\n",
    "In the Markov Game (MG) formalization of reinforcement learning, multiple adaptive agents interact within a shared environment. Each agent aims to maximize its expected cumulative reward, which may depend on the actions of other agents. It is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{M} = (\\mathcal{N}, \\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{N} &: \\text{ Set of n agents } \\\\\n",
    "\\mathcal{S} &: \\text{ States of the shared environment} \\\\\n",
    "\\mathcal{A} &: \\text{ The action set, where each } A_i \\text{ is the action set of agent } i \\in N \\\\\n",
    "a = (a_1, a_2, ..., a_n) &: \\text{ The joint action taken by all agents in state } s \\\\\n",
    "P(s' \\mid s, a) &: \\text{ The transition probability function, returning the probability of transitioning to state } s' \\\\\n",
    "&\\quad \\text{ given state } s \\text{ and joint actions } A_t \\\\\n",
    "R_i(s, a) &: \\text{ The reward function of agent } i, \\text{ mapping states and joint actions to rewards } \\\\\n",
    "R &: \\text{ The set of reward functions } R = \\{R_1, \\dots, R_N\\}. \\\\\n",
    "\\gamma &: \\text{ Discount factor, determining the importance of future rewards }\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The policy function for agent \\(i\\) is extended to a joint policy function for all agents, defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\pi}(\\mathbf{a} \\mid s) = \\prod_{i=1}^N \\pi_i(a_i \\mid s),\n",
    "$$\n",
    "\n",
    "The value function for agent \\(i\\) is similarly extended to incorporate the joint policy and joint actions, defined as:\n",
    "\n",
    "$$\n",
    "V_i(s) = \\mathbb{E}_{\\boldsymbol{\\pi}} \\left[ \\sum_{t=0}^\\infty \\gamma^t \\mathcal{R}_i(s_t, \\mathbf{a}_t) \\mid s_0 = s \\right].\n",
    "$$\n",
    "\n",
    "In the case of zero-sum games, the value function satisfies:\n",
    "\n",
    "$$\n",
    "V_1(s) = -V_2(s),\n",
    "$$\n",
    "\n",
    "indicating that the gain of one agent is the loss of the other. In cooperative settings, the value function may represent a shared cumulative reward:\n",
    "\n",
    "$$\n",
    "V(s) = \\sum_{i=1}^N V_i(s),\n",
    "$$\n",
    "\n",
    "capturing the collective outcome of all agents acting together.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
