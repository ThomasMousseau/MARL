{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2iXfk_yueBG"
   },
   "source": [
    "##### INF8250 – Reinforcement Learning - Fall 2024 - Final Project\n",
    "\n",
    "# **Multi-Agent Reinforcement Learning**\n",
    "\n",
    "### Members:\n",
    "\n",
    "- Alexandre Fournier - 2147771\n",
    "\n",
    "- Thomas Mousseau - 2149672\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkcXOcIvvc70"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Multi-Agent Reinforcement Learning (MARL) extends the RL framework to environments where multiple agents interact, each potentially learning and adapting simultaneously. In MARL, agents may cooperate, compete, or both, adding complexity to the learning process. Understanding MARL is crucial for developing systems where multiple autonomous entities need to operate in shared environments, such as swarm robotics, autonomous driving, and distributed control systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v4BCtgvvk0q"
   },
   "source": [
    "## 1. From Single-Agent MDP to Multi-Agent Markov Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRX136vA1I6j"
   },
   "source": [
    "### Markov Decision Process (MDP)\n",
    "\n",
    "In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment with the objective of maximizing the expected cumulative reward over time. The framework is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, r, \\gamma)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{S} &: \\text{ States of the environment} \\\\\n",
    "\\mathcal{A} &: \\text{ Actions set of the agent } \\\\\n",
    "\\mathcal{a} &: \\text{ The action taken by the agent in state s } \\\\\n",
    "\\mathcal{P}(s' \\mid s, a) &: \\text{ Transition probability for reaching state } s' \\text{ from } s \\text{ after action } a \\\\\n",
    "r(s, a) &: \\text{ Reward for taking action } a \\text{ in state } s \\\\\n",
    "\\gamma &: \\text{ Discount factor, determining the importance of future rewards }\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The agent’s policy function $\\pi(a \\mid s)$ defines the probability of selecting action $(a)$ in state $(s)$, while the value function $V(s)$ represents the expected cumulative reward starting from $(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhSmGEKH0j1Q"
   },
   "source": [
    "### Markov Game (MG)\n",
    "\n",
    "In the Markov Game (MG) formalization of reinforcement learning, multiple adaptive agents interact within a shared environment. Each agent aims to maximize its expected cumulative reward, which may depend on the actions of other agents. It is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{M} = (\\mathcal{N}, \\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{N} &: \\text{ Set of n agents } \\\\\n",
    "\\mathcal{S} &: \\text{ States of the shared environment} \\\\\n",
    "\\mathcal{A} &: \\text{ The action set, where each } A_i \\text{ is the action set of agent } i \\in N \\\\\n",
    "a = (a_1, a_2, ..., a_n) &: \\text{ The joint action taken by all agents in state } s \\\\\n",
    "P(s' \\mid s, a) &: \\text{ The transition probability function, returning the probability of transitioning to state } s' \\\\\n",
    "&\\quad \\text{ given state } s \\text{ and joint actions } A_t \\\\\n",
    "R_i(s, a) &: \\text{ The reward function of agent } i, \\text{ mapping states and joint actions to rewards } \\\\\n",
    "R &: \\text{ The set of reward functions } R = \\{R_1, \\dots, R_N\\}. \\\\\n",
    "\\gamma &: \\text{ Discount factor, determining the importance of future rewards }\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The policy function for agent \\(i\\) is extended to a joint policy function for all agents, defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\pi}(\\mathbf{a} \\mid s) = \\prod_{i=1}^N \\pi_i(a_i \\mid s),\n",
    "$$\n",
    "\n",
    "The value function for agent \\(i\\) is similarly extended to incorporate the joint policy and joint actions, defined as:\n",
    "\n",
    "$$\n",
    "V_i(s) = \\mathbb{E}_{\\boldsymbol{\\pi}} \\left[ \\sum_{t=0}^\\infty \\gamma^t \\mathcal{R}_i(s_t, \\mathbf{a}_t) \\mid s_0 = s \\right].\n",
    "$$\n",
    "\n",
    "In the case of zero-sum games, the value function satisfies:\n",
    "\n",
    "$$\n",
    "V_1(s) = -V_2(s),\n",
    "$$\n",
    "\n",
    "indicating that the gain of one agent is the loss of the other. In cooperative settings, the value function may represent a shared cumulative reward:\n",
    "\n",
    "$$\n",
    "V(s) = \\sum_{i=1}^N V_i(s),\n",
    "$$\n",
    "\n",
    "capturing the collective outcome of all agents acting together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlphaStar: Achieving Grandmaster Level in StarCraft II\n",
    "\n",
    "In 2019, Google DeepMind took on yet another ambitious challenge in the realm of video games: conceiving an agent capable of competing with the best players in the world. Following their groundbreaking success with AlphaGo, their next target became StarCraft II, a highly complex and strategic real-time strategy game.\n",
    "\n",
    "Background\n",
    "\n",
    "Among the many innovations, network architectures, and techniques used, some novel ideas regarding multi-agent reinforcement learning (MARL) emerged, proving instrumental to AlphaStar’s success. Before diving into these concepts, it’s important to first understand the problem that necessitated such solutions.\n",
    "\n",
    "A StarCraft II match between two players represents a competitive setting, where the efficacy of self-play had already been demonstrated in prior research. Self-play, however, fosters a cyclic nature of learning: strategies may improve temporarily but are prone to falling into pseudo-improvement loops, akin to endlessly iterating between the game of rock-paper-scissors. To counter this limitation, DeepMind introduced exploiters—dedicated agents trained to identify weaknesses in the strongest models and expose vulnerabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaStar: Achieving Grandmaster Level in StarCraft II\n",
    "\n",
    "In 2019, Google DeepMind took on yet another ambitious challenge in the realm of video games: conceiving an agent capable of competing with the best players in the world. Following their groundbreaking success with **AlphaGo**, their next target became **StarCraft II**, a highly complex and strategic real-time strategy game.\n",
    "\n",
    "## Background\n",
    "\n",
    "Among the many innovations, network architectures, and techniques used, some novel ideas regarding **multi-agent reinforcement learning** (MARL) emerged, proving instrumental to AlphaStar's success. Before diving into these concepts, it’s important to first understand the problem that necessitated such solutions.\n",
    "\n",
    "A StarCraft II match between two players represents a **competitive setting**, where the efficacy of **self-play** had already been demonstrated in prior research. Self-play, however, fosters a **cyclic nature** of learning: strategies may improve temporarily but are prone to falling into **pseudo-improvement loops**, akin to endlessly iterating between the game of rock-paper-scissors. To counter this limitation, DeepMind introduced **exploiters**, dedicated agents trained to identify weaknesses in the strongest models and expose vulnerabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Loop and Role of Exploiters\n",
    "\n",
    "The **training loop** is structured as follows:\n",
    "\n",
    "1. **Main Agents**:  \n",
    "   These agents are trained to improve against all **past players**, snapshotted versions of agents saved at various points during training. This ensures that progress builds upon prior strategies and knowledge.\n",
    "\n",
    "2. **League Exploiters**:  \n",
    "   These agents train specifically against all past players to uncover **systemic weaknesses** within the entire league. Their role is to push the league to address strategies that might not be robust when generalized.\n",
    "\n",
    "3. **Main Exploiters**:  \n",
    "   Unlike league exploiters, main exploiters train **solely against the current main agents** to identify **specific exploits** in their strategies. This forces the main agents to adapt and eliminate weaknesses.\n",
    "\n",
    "To maintain diversity and prevent stagnation, both **main exploiters** and **league exploiters** are periodically **reinitialized**. This reset allows for exploration of new strategies and ensures that exploiters don’t overfit to particular weaknesses.\n",
    "\n",
    "---\n",
    "\n",
    "## Prioritized Fictitious Self-Play (PFSP)\n",
    "\n",
    "A core component of this framework is **prioritized fictitious self-play (PFSP)**. Rather than uniformly sampling opponents (as in traditional self-play), PFSP uses a **weighted mixture** of opponents, prioritizing those against which the agent struggles most. This ensures the main agents continuously learn to overcome their toughest adversaries while still retaining lessons learned from past strategies.\n",
    "\n",
    "The **periodic snapshots** of agents into the league, combined with the PFSP mechanism, drive steady progress. By emphasizing challenging opponents and integrating exploiters into the training pool, DeepMind ensured that **AlphaStar** avoided stagnation and achieved a high level of robustness.\n",
    "\n",
    "---\n",
    "\n",
    "## Empirical Results\n",
    "\n",
    "The success of these innovations is reflected in the empirical results:\n",
    "- Without the addition of exploiters into the training process, the main agents failed to reach high Elo scores.  \n",
    "- The inclusion of exploiters, coupled with PFSP, enabled AlphaStar to achieve a rating of **1600 Elo** and beyond.  \n",
    "- Ablation studies, such as **graph d**, demonstrated that combining **self-play** with **prioritized fictitious self-play** yielded the **best results**, outperforming alternative methods.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "DeepMind’s AlphaStar introduced groundbreaking concepts in **multi-agent reinforcement learning**, particularly through the innovative use of **league training**, **exploiters**, and **prioritized fictitious self-play**. By maintaining a dynamic league of diverse opponents and continuously addressing strategic weaknesses, AlphaStar achieved a robust and adaptive learning process.\n",
    "\n",
    "These contributions not only allowed AlphaStar to reach **Grandmaster level** in StarCraft II but also showcased the potential of MARL to solve real-world, complex problems requiring strategic and adaptive decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
